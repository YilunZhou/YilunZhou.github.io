<p>My research is broadly in the area of trustworthy and responsible AI, or how to deploy AI systems, especially black-box models such as neural networks, to be dependable and reliable to end users. In particular, I approach this problem from the perspective of model interpretability, or generating explanations for individual model predictions, such as highlighting important pixels that explains why an image classifier predicts an image to be a cat rather than a dog. </p>

<p>For a large part of my thesis, I have been focusing on the evaluation of these explanations, to ensure that they can really be trusted to provide the correct model understanding. This problem is often overlooked, and people take whatever the explanation tells them to be true. For example, in many medical journal papers, you see that people propose some state of the art model for data-driven medical diagnosis, and then analyze the model using some explanation methods, basically assuming that they are correct. </p>

<p>Instead, I question the correctness assumptions, and propose evaluations that rigorously test them. But this evaluation is quite tricky. If we think about it, the very reason why we use these explanation methods is because we can't directly understand our model in the first place, but this means that any sort of definitive evaluation is impossible due to our inherent lack of understanding. Thus, in our recent AAAI paper, we proposed a ground truth based evaluation technique, of modifying a dataset by injecting features in a specific way and retraining the model, such that we know what features are important to the model, and then comparing whether the explanation methods can highlight our expected features. The features that we inject look mostly like spurious correlations, which have been a concern for many real world datasets and models. Alarmingly, we found that many popular explanation techniques cannot reliably identify the strong impact of these features, casting doubt on their practical capabilities of spurious correlation detection in the wild. </p>

<p>Beyond correctness, we proposed an orthogonal concept in our NAACL paper, which we call "understandability". Given that the explanations are correct, can we really correctly and consistently understand them? Or are we instead hallucinating more general and global properties of the models simply because they seem plausible while not being actually supported by the explanations? The latter seems especially likely as these deep neural networks are fundamentally complex machines, working possibly in ways unimaginable to human. We propose a mathematical framework for characterizing and quantifying this understandability aspect, and found that exiting local explanations are indeed very prone to be misunderstood if not being careful. </p>

<p>Finally, my latest and current works try to propose new explanation methods so that they are more correct and understandable, so that they can be really trusted to provide high-quality model understanding. And going back to the big picture of trustworthy and responsible AI, I really see interpretability as a means than an end, the means of achieving holistic understanding and making better informed decisions to make these systems more trustworthy in general, for example. Thus, I am also very interested in exploring the connections between interpretability and areas such as robustness, fairness and privacy. </p>
